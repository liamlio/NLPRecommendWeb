{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoNaturalLanguageRecommendationsSimpleDemoCPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/NaturalLanguageRecommendations/blob/master/notebooks/inference/DemoNaturalLanguageRecommendationsSimpleDemoCPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9eERu8JP9r7",
        "colab_type": "text"
      },
      "source": [
        "This is our simple Colab demo notebook, which can run on a CPU instance, though it may crash a regular colab CPU instance since it's memory intensive. If that happens, a message should appear on the bottom left asking if you would like to switch to a 25 gb RAM instance, which will be more than enough memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3YuDoc3QUBv",
        "colab_type": "text"
      },
      "source": [
        "If you would like play with using a GPU or TPU for inference, please see our advanced demo notebook here https://colab.research.google.com/github/Santosh-Gupta/NaturalLanguageRecommendations/blob/master/notebooks/inference/build_index_and_search.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8wAw2JV6F4Am",
        "outputId": "2cf2ba48-5db2-4630-a361-a7f61cfe3a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "cellView": "both"
      },
      "source": [
        "#@title Download and load model, embeddings, and data, will take a several minutes. Double click on this to pop open the hood and checkout the code.\n",
        "\n",
        "!gdown --id \"10LV9QbZOkUyOzR4nh8hxesoKJhpmvpM9\"   # citation vectors\n",
        "!gdown --id \"1-8gmT9cQpOUoZ_HzEaT9Xz6qfeVooAFn\"\n",
        "!gdown --id \"1-23aNm7j0bnycvyd_OaQfofVYPTewgOI\"   # abstract vectors\n",
        "!gdown --id \"1NyUQwgUNj9bFsiCnZ2TfKmWn5r-Y6wav\"   # TitlesIdAbstractsEmbedIds\n",
        "!wget 'https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar'\n",
        "!tar -xvf 'scibert_scivocab_uncased.tar'\n",
        "\n",
        "!pip install transformers --quiet\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "print('TensorFlow:', tf.__version__)\n",
        "\n",
        "print('Loading Embeddings')\n",
        "citations_embeddings = np.load('CitationSimilarityVectors106Epochs.npy')\n",
        "abstract_embeddings = np.load('AbstractSimVectors.npy')\n",
        "assert citations_embeddings.shape == abstract_embeddings.shape\n",
        "\n",
        "normalizedC = tf.nn.l2_normalize(citations_embeddings, axis=1)\n",
        "normalizedA = tf.nn.l2_normalize(abstract_embeddings, axis=1) \n",
        "\n",
        "print('Loading Model')\n",
        "model = tf.saved_model.load('gs://tfworld/saved_models')\n",
        "print('laoding Tokenizer')\n",
        "tokenizer = BertTokenizer(vocab_file='scibert_scivocab_uncased/vocab.txt')\n",
        "\n",
        "print('Loading Semantic Scholar CS data. This will take a few more minutes, almost done . . .')\n",
        "df = pd.read_json('/content/TitlesIdsAbstractsEmbedIdsCOMPLETE_12-30-19.json.gzip', compression = 'gzip')\n",
        "embed2Title = pd.Series(df['title'].values,index=df['EmbeddingID']).to_dict()\n",
        "embed2Abstract = pd.Series(df['paperAbstract'].values,index=df['EmbeddingID']).to_dict()\n",
        "embed2Paper = pd.Series(df['id'].values,index=df['EmbeddingID']).to_dict()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10LV9QbZOkUyOzR4nh8hxesoKJhpmvpM9\n",
            "To: /content/CitationSimilarityVectors106Epochs.npy\n",
            "2.59GB [00:24, 107MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-8gmT9cQpOUoZ_HzEaT9Xz6qfeVooAFn\n",
            "To: /content/CombAfterNormalization106Epochs.npy\n",
            "2.59GB [00:43, 59.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-23aNm7j0bnycvyd_OaQfofVYPTewgOI\n",
            "To: /content/AbstractSimVectors.npy\n",
            "2.59GB [00:42, 60.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NyUQwgUNj9bFsiCnZ2TfKmWn5r-Y6wav\n",
            "To: /content/TitlesIdsAbstractsEmbedIdsCOMPLETE_12-30-19.json.gzip\n",
            "432MB [00:06, 67.6MB/s]\n",
            "--2020-01-01 04:14:48--  https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.216.224\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.216.224|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 442460160 (422M) [application/x-tar]\n",
            "Saving to: ‘scibert_scivocab_uncased.tar’\n",
            "\n",
            "scibert_scivocab_un 100%[===================>] 421.96M  37.1MB/s    in 12s     \n",
            "\n",
            "2020-01-01 04:15:01 (35.0 MB/s) - ‘scibert_scivocab_uncased.tar’ saved [442460160/442460160]\n",
            "\n",
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/vocab.txt\n",
            "scibert_scivocab_uncased/pytorch_model.bin\n",
            "scibert_scivocab_uncased/config.json\n",
            "\u001b[K     |████████████████████████████████| 450kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 860kB 9.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 12.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "TensorFlow 2.x selected.\n",
            "TensorFlow: 2.1.0-rc1\n",
            "Loading Model\n",
            "laoding Tokenizer\n",
            "Loading Semantic Scholar CS data. This will take a few more minutes, almost done . . .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVzix3jhPIYx",
        "colab_type": "text"
      },
      "source": [
        "Use this text box to search for papers. Our model was trained on using full abstracts using the 'query', so the model performs better with longer queries, but the model works surprisingly well with short queries as well. Give it a try."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lMVIjtwQm-1",
        "colab_type": "text"
      },
      "source": [
        "Our model was trained to use a citation emedding as a label, but we found out running similarity on our abstract embeddings results in surprisingly robust results as well, so we included both. The first half of the results are from the citation embeddings, the second half are from the abstract embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJJnXM5BiE-m",
        "colab_type": "code",
        "outputId": "9ec41fd3-61f8-4487-8cc9-27e689457c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form"
      },
      "source": [
        "query = \"Model for extracting an interpretable sentence embedding with self-attention. \" #@param {type:\"string\"}\n",
        "\n",
        "top_k_results = 20 #@param {type:\"integer\"}\n",
        "\n",
        "if top_k_results%2 == 0:\n",
        "    halfA = halfC = int(top_k_results/2)\n",
        "else:\n",
        "    halfC = int(top_k_results/2) + 1\n",
        "    halfA = int(top_k_results/2) \n",
        "\n",
        "abstract_encoded = tokenizer.encode(query, max_length=512, pad_to_max_length=True)\n",
        "abstract_encoded = tf.constant(abstract_encoded, dtype=tf.int32)[None, :]\n",
        "print('\\nQuery : ')\n",
        "pprint(query)\n",
        "\n",
        "s = time()\n",
        "bert_output = model(abstract_encoded)\n",
        "xq = tf.nn.l2_normalize(bert_output, axis=1)\n",
        "prediction_time = time() - s\n",
        "\n",
        "simNumpyC = np.matmul(normalizedC, tf.transpose(xq))\n",
        "simNumpyCTopK = (-simNumpyC[:,0]).argsort()[:halfC]\n",
        "simNumpyC_oTopK = -np.sort(-simNumpyC[:,0])[:halfC]\n",
        "allCit = np.vstack((simNumpyCTopK , simNumpyC_oTopK) )\n",
        "del simNumpyC\n",
        "\n",
        "simNumpyA = np.matmul(normalizedA, tf.transpose(xq))\n",
        "simNumpyATopK = (-simNumpyA[:,0]).argsort()[:halfA]\n",
        "simNumpyA_oTopK = -np.sort(-simNumpyA[:,0])[:halfA]\n",
        "allAbs = np.vstack((simNumpyATopK , simNumpyA_oTopK) )\n",
        "del simNumpyA\n",
        "\n",
        "allResults = np.concatenate((allAbs, allCit), axis = 1)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('------ Nearest papers  -----------------------------------------------------------')\n",
        "print('\\n')\n",
        "\n",
        "for embed in allResults[0]:\n",
        "    print('---------------')\n",
        "    print('-------')\n",
        "    print('---')\n",
        "    title = embed2Title[int(embed)]\n",
        "    abstractR = embed2Abstract[int(embed)]\n",
        "    paperId = embed2Paper[int(embed)]\n",
        "    print('Title: ', title)\n",
        "    print('\\nAbstract : ')\n",
        "    pprint(abstractR)\n",
        "    # print('\\n')\n",
        "    print('\\nLink: https://www.semanticscholar.org/paper/'+paperId)\n",
        "    print('---')\n",
        "    print('-------')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Query : \n",
            "'Model for extracting an interpretable sentence embedding with self-attention. '\n",
            "\n",
            "\n",
            "------ Nearest papers  -----------------------------------------------------------\n",
            "\n",
            "\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  iSentenizer: An incremental sentence boundary classifier\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we revisited the topic of sentence boundary detection, and '\n",
            " 'proposed an incremental approach to tackle the problem. The boundary '\n",
            " 'classifier is revised on the fly to adapt to the text of high variety of '\n",
            " 'sources and genres. We applied i+Learning, an incremental algorithm, for '\n",
            " 'constructing the sentence boundary detection model using different features '\n",
            " 'based on local context. Although the model can be easily trained on any '\n",
            " 'genre of text and on any alphabet language, we emphasize the ability that '\n",
            " 'the classifier is adaptable to text with domain and topic shifts without '\n",
            " 'retraining the whole model from scratch. Empirical results indicate that the '\n",
            " 'performance of proposed system is comparable to that of similar systems.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/3181562028ecab8692501aa04b5f6e1e0fead183\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Improved Sequential Dependency Analysis Integrating Labeling-Based Sentence Boundary Detection\n",
            "\n",
            "Abstract : \n",
            "('A dependency structure interprets modification relationships between words '\n",
            " 'or phrases and is recognized as an important element in semantic information '\n",
            " 'analysis. With the conventional approaches for extracting this dependency '\n",
            " 'structure, it is assumed that the complete sentence is known before the '\n",
            " 'analysis starts. For spontaneous speech data, however, this assumption is '\n",
            " 'not necessarily correct since sentence boundaries are not marked in the '\n",
            " 'data. Although sentence boundaries can be detected before dependency '\n",
            " 'analysis, this cascaded implementation is not suitable for online processing '\n",
            " 'since it delays the responses of the application. To solve these problems, '\n",
            " 'we proposed a sequential dependency analysis (SDA) method for online '\n",
            " 'spontaneous speech processing, which enabled us to analyze incomplete '\n",
            " 'sentences sequentially and detect sentence boundaries simultaneously. In '\n",
            " 'this paper, we propose an improved SDA integrating a labeling-based sentence '\n",
            " 'boundary detection (SntBD) technique based on Conditional Random Fields '\n",
            " '(CRFs). In the new method, we use CRF for soft decision of sentence '\n",
            " 'boundaries and combine it with SDA to retain its online framework. Since '\n",
            " 'CRF-based SntBD yields better estimates of sentence boundaries, SDA can '\n",
            " 'provide better results in which the dependency structure and sentence '\n",
            " 'boundaries are consistent. Experimental results using spontaneous lecture '\n",
            " 'speech from the Corpus of Spontaneous Japanese show that our improved SDA '\n",
            " 'outperforms the original SDA with SntBD accuracy providing better dependency '\n",
            " 'analysis results.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/816bc34c08e6bb8baa01e4f317d4aaadb3a604bd\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  The Effect of POS Tag Information on Sentence Boundary Detection in Turkish Texts\n",
            "\n",
            "Abstract : \n",
            "('Recently, Natural language processing (NLP) applications have been crucial '\n",
            " 'by the increase in the amount of digitized written and oral text documents. '\n",
            " 'As sentence boundary detection is the first step of most of the NLP '\n",
            " 'applications, it has high importance. In this study, the effects of using '\n",
            " 'POS (Part-of-Speech) tags on the performance of machine learning '\n",
            " 'methodsbased sentence boundary detection from Turkish texts have been '\n",
            " 'studied. To reach our goal, a dataset which contains 30000 instances such '\n",
            " 'that 15000 of them are sentences and the remaining 15000 instances are '\n",
            " 'non-sentence samples has been drawn from a subset of TNC (Turkish National '\n",
            " 'Corpus). The sub-corpus has 10.000.000 words in total, and to develop the '\n",
            " 'dataset, the characters which may represent the end of a sentence are '\n",
            " 'searched from the sub-corpus, then the text is divided into pieces from '\n",
            " 'these characters. Each piece is checked manually to label as sentence and '\n",
            " 'non-sentence, and randomly 30000 instances are selected to form the dataset. '\n",
            " 'Each instance in the dataset is converted to a vector by using total 9 '\n",
            " 'attributes that are used in the rule-based sentence boundary detection '\n",
            " 'studies and proposed in this study. After that two more attributes that are '\n",
            " 'POS tags of the terms before and after the haracter that may represent the '\n",
            " 'end of the sentence are included to the attribute set, and then the dataset '\n",
            " 'is again converted to vectors by using these 11 attributes. The twodatasets '\n",
            " 'are classified by using Back Propagation Neural Network, RBF Network, Naive '\n",
            " 'Bayes classifier, Decision Tree, and Support Vector Machines to evaluate the '\n",
            " 'performance of supervised learning methods on the sentence boundary '\n",
            " 'detection. After the experimental evaluation we observed that, when POS tags '\n",
            " 'are included, success of sentence boundary detection increases for all '\n",
            " 'classifiers, and the most successful classifier is decision tree with '\n",
            " 'classification accuracy which is improved from 84.7% to 86.2% when POS tags '\n",
            " 'are considered.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/14b31ff3917b32e6d15f1c04a734e32e82a5b138\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  A New Feature Extraction Approach Based on Sentence Element Analysis\n",
            "\n",
            "Abstract : \n",
            "('Considering that each sentence element of a sentence or clause plays an '\n",
            " 'important role in describing case and (or) object in documents, a feature '\n",
            " 'extraction method based on sentence element is proposed in this paper. The '\n",
            " 'method can extract feature terms from documents effectively and weight them '\n",
            " 'accurately. It first extracts sentence elements from dependency '\n",
            " 'relationships, and then selects and weights the terms according to the '\n",
            " 'sentence elements. Experimental results on a public dataset prove the '\n",
            " 'feasibility of our approach and demonstrate its advantage to feature '\n",
            " 'extraction method based on part of speech.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/d9cb323ad64f6b6cddfcb6e706df054a8f91e3ae\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Encoding Local Contexts of Sentences with Convolutions on pq-Gram Representations of Dependency Trees\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we present our sentence encoding approach that uses local '\n",
            " 'contexts constructed from pq-gram representations of a sentence’s dependency '\n",
            " 'tree. The context localization scope can be adjusted through parameters p, '\n",
            " 'the dependency depth, and q, the dependency width, which allows controlling '\n",
            " 'context-sensitivity. We show competitive results of using our sentence '\n",
            " 'encoding approach for sentence-pair modeling tasks.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/c090d6dcb7ec31edd7688a0a3628c68a686502e1\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  The analysis on mistaken segmentation of Tibetan words based on statistical method\n",
            "\n",
            "Abstract : \n",
            "('In this paper, by using the Tibetan word segmentation system, IEA-TWordSeg, '\n",
            " 'the authors attempt segmentation of the total 1271 sentences in the closed '\n",
            " 'set and 1000 sentences in an open set. The accuracy of testing is 99.54% and '\n",
            " '92.41% respectively. The authors describe the wrong segmentation types as '\n",
            " 'well as the causes of the mistakes, and demonstrate the proportion of '\n",
            " 'different types of segmentation errors. The purpose of the article is to '\n",
            " 'provide clues for those who intend to improve the accuracy of Tibetan word '\n",
            " 'segmentation system.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/a3f876619b321285c61244d5f8501c9ca4359cff\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Semantic role based sentence compression\n",
            "\n",
            "Abstract : \n",
            "('In this paper, a new unsupervised sentence compression method is proposed. '\n",
            " 'Sentences are tagged with Part Of Speech tags and semantic role labels. The '\n",
            " \"proposed method relies on the semantic roles of sentences' parts. Moreover, \"\n",
            " 'in the process of compression, other sentences in the context are taken into '\n",
            " 'account. The approach is applied in the context of multi-document '\n",
            " 'summarization. Experiments showed better results than other state of the art '\n",
            " 'approaches.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/c1b262231f1cd2dd38224a170123063980dc3750\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Extraction and categorization of transition information from large volume of texts using patterns and machine learning\n",
            "\n",
            "Abstract : \n",
            "('The information on transition of a thing is important for acquiring '\n",
            " 'knowledge on the thing. In this study, we extracted transition information '\n",
            " 'from a large number of sentences using pattern-based methods. We obtained an '\n",
            " 'F-measure of 0.86 for extraction of transition information by this method. '\n",
            " 'In order to improve the results, we then combined the pattern-based methods '\n",
            " 'with supervised machine-learning methods. We obtained F-measures of 0.91 and '\n",
            " '0.89 for extraction of transition information by support vector machine and '\n",
            " 'maximum entropy methods, respectively. Thus, we confirmed that the combined '\n",
            " 'approach outperformed the pattern-based method. We also categorized '\n",
            " 'transition information. In the experiments, we obtained an F-measure of 0.6 '\n",
            " 'for transition information categorization for categories containing many '\n",
            " 'events in the training data set. Furthermore, we examined sentences in terms '\n",
            " 'of conceptual changes in their transition information. We classified '\n",
            " 'transition information in various ways. The categories by the classification '\n",
            " 'provide a theoretical basis for future studies on transition information.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/66a3f6ad8c98ba24cf69957d22da65a7029d252d\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Legal Clause Extraction From Contract Using Machine Learning with Heuristics Improvement\n",
            "\n",
            "Abstract : \n",
            "('In this paper we are going to motivate and propose a novel task: Legal '\n",
            " 'Clause extraction from a contract document. For clause extraction, we '\n",
            " 'perform paragraph segmentation using paragraph boundary detection. In this '\n",
            " 'paper, we have discussed conventional paragraph detection approaches and '\n",
            " 'then describe the machine learning based approach. We detect paragraph '\n",
            " 'boundary using sequential classification algorithm. We also added varieties '\n",
            " 'of text feature which help to improve the accuracy of paragraph boundary '\n",
            " 'detection. To improve the accuracy of the model we have added false positive '\n",
            " 'samples and create a revised model. We introduce the legal clause extraction '\n",
            " 'system that performs the task with high accuracy.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/5142418193fb4fb21d46ac9d44479cb2816b30a9\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Representing sentence with unfolding recursive autoencoders and dynamic average pooling\n",
            "\n",
            "Abstract : \n",
            "('This paper proposes a new composition method to represent semantic '\n",
            " 'compositionality of sentences. Using the unfolding recursive autoencoders, '\n",
            " 'we build sentence representing trees from the original sentences of words. '\n",
            " 'We utilize trained word embeddings and sentence parser to train the model, '\n",
            " 'and we can build sentence representing trees from the trained model. We '\n",
            " 'further propose to use dynamic average pooling to pool the trees and get '\n",
            " 'fix-size vector representation for sentences. The fix-size vector '\n",
            " 'representation after dynamic average pooling can then be used to represent '\n",
            " 'sentences. We verify the validity of sentence representations by using them '\n",
            " 'to classify sentence paraphrase. Experiment shows that compared to the '\n",
            " 'baseline representation, using proposed representations together with '\n",
            " 'sub-vector Euclidean distance feature, the classification performance can be '\n",
            " 'improved by 1.39% for testing accuracy, which proves the proposed method can '\n",
            " 'better represent semantic compositionality of sentences.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/4e0a61b300408e721a3b0dad7dbd88a8f9587ece\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Hovering Controller Design for a Helicopter\n",
            "\n",
            "Abstract : \n",
            "('This paper studies the hovering controller design for a helicopter. The '\n",
            " 'linear helicopter motion equation is introduced. The hovering control law is '\n",
            " 'designed and the parameters mapping method is employed to search the best '\n",
            " 'available hover controller parameters in all flight states. The digital '\n",
            " 'simulation models are established. Simulation results are given to '\n",
            " 'demonstrate the effectiveness of the designed controllers. The hover '\n",
            " 'maneuver controlled by the designed controller passed the test flight.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/1b6538180e6621cec68ca9187ff2c68f33210824\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Pseudo-Identities and Bordered Words\n",
            "\n",
            "Abstract : \n",
            "('This paper investigates the notions of θ-bordered words and θ-unbordered '\n",
            " 'words for various pseudo-identity functions θ. A θ-bordered word is a '\n",
            " 'non-empty word u such that there exists a word v which is a prefix of u '\n",
            " 'while θ(v) is a suffix of u. The case where θ is the identity function '\n",
            " 'corresponds to the classical notions of bordered and unbordered words. Here '\n",
            " 'we explore cases where θ is a pseudo-identity function, such as a morphism '\n",
            " 'or antimorphism with the property θ = I, n ≥ 2, or a literal morphism or '\n",
            " 'antimorphism. We explore properties of θ-bordered and θ-unbordered words in '\n",
            " 'this context.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/8b638e8cb4bf31047de2629eceee91c457616a2e\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Effects of Adaptation Parameters on Convergence Time and Tolerance for Adaptive Threshold Elements\n",
            "\n",
            "Abstract : \n",
            "''\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/43150b8951f89c9a43e6ff8b04771625daf9c7f4\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Automatic segmentation of news items based on video and audio features\n",
            "\n",
            "Abstract : \n",
            "('The automatic segmentation of news items is a key for implementing the '\n",
            " 'automatic cataloging system of news video. This paper presents an approach '\n",
            " 'which manages audio and video feature information to automatically segment '\n",
            " 'news items. The integration of audio and visual analyses can overcome the '\n",
            " 'weakness of the approach using only image analysis techniques. It makes the '\n",
            " 'approach more adaptable to various situations of news items. The proposed '\n",
            " 'approach detects silence segments in accompanying audio, and integrates them '\n",
            " 'with shot segmentation results, as well as anchor shot detection results, to '\n",
            " 'determine the boundaries among news items. Experimental results show that '\n",
            " 'the integration of audio and video features is an effective approach to '\n",
            " 'solving the problem of automatic segmentation of news items.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/fb74daee751f2ef3a0a5b4734649c9d8e14bc332\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Word Semantic Similarity Calculation Based on Word2vec\n",
            "\n",
            "Abstract : \n",
            "('In order to solve the problem of poor universality and the absence of '\n",
            " 'contextual information in word similarity calculation based on dictionary, '\n",
            " 'this paper proposes a semantic similarity computation method based on '\n",
            " 'Word2vec. This method improves HowNet and Tongyici Cilin, and also adds the '\n",
            " 'word vector model as a weighing parameter to calculate the word similarity, '\n",
            " 'after compares the similarity of the words by assigning different weights to '\n",
            " 'the three methods. Through experimental comparison, the Pearson coefficient '\n",
            " 'of the algorithm and the artificial value is 0.892, and the method can cover '\n",
            " 'most words so that it can effectively solve the problem of the similarity of '\n",
            " 'the word calculation in the dictionary.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/0d7e3191b05d94b9ed24fa16c600527b1312286c\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  A texture-based method for document segmentation and classification\n",
            "\n",
            "Abstract : \n",
            "('In this paper we present a hybrid approach to segment and classify contents '\n",
            " 'of document images. A Document Image is segmented into three types of '\n",
            " 'regions: Graphics, Text and Space. The image of a document is subdivided '\n",
            " 'into blocks and for each block five GLCM (Grey Level Co-occurrence Matrix) '\n",
            " 'features are extracted. Based on these features, blocks are then clustered '\n",
            " 'into three groups using K-Means algorithm; connected blocks that belong to '\n",
            " 'the same group are merged. The classification of groups is done using '\n",
            " 'pre-learned heuristic rules. Experiments were conducted on scanned '\n",
            " 'newspapers and images from MediaTeam Document Database')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/33ae0c2ae9e7ad1d6fa3fcc82e1bd5a6c8204eba\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  An EMD-based long-term LSSVR for machine condition prognostics\n",
            "\n",
            "Abstract : \n",
            "('Machine condition prognostics is very important for system safety and '\n",
            " 'condition-based maintenance. Only one-step ahead forecasting of machine '\n",
            " 'condition is considered because of the poor performance of multi-step ahead '\n",
            " 'forecasting. To find effective method for non-stationary long-term machine '\n",
            " 'condition prognostics, this paper firstly reviews multi-step ahead '\n",
            " 'forecasting strategies followed with a comparison of different multi-step '\n",
            " 'ahead forecasting strategies using support vector regression (SVR) and least '\n",
            " 'squares support vector regression (LSSVR) in two datasets, and then develops '\n",
            " 'an recursive multi-step LSSVR (MSLSSVR) model by introducing empirical mode '\n",
            " 'decomposition (EMD) to realize machine condition prognostics. EMD is used to '\n",
            " 'get more stationary signals instead of the non-stationary original signal, '\n",
            " 'and MSLSSVR is constructed to make multi-step prediction with decomposed '\n",
            " 'signals individually. All predicted values are combined eventually to get '\n",
            " 'the future trajectory of condition indicator. Moreover, the proposed '\n",
            " 'algorithm is validated in the TE process. The experimental result shows that '\n",
            " 'the proposed EMD-MSLSSVR model can forecast the failure status in advance, '\n",
            " 'and the performance is satisfied.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/6d0d24d395488f84716dde54a8e322b9969a9afa\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Performance Analysis of Adaptive Processors via Diffusion Model Approximations and the Fokker-Planck Equation\n",
            "\n",
            "Abstract : \n",
            "('Some results on adaptive array processors and adaptive filters that have '\n",
            " 'been obtainsd by the use of diffusion model approximations and the '\n",
            " 'Fokker-Planck equation are described.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/2f0b49211312c80eefbd4be254bb0361441dca8a\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  On a special class of primitive words\n",
            "\n",
            "Abstract : \n",
            "('When representing DNA molecules as words, it is necessary to take into '\n",
            " 'account the fact that a word u encodes basically the same information as its '\n",
            " 'Watson-Crick complement @q(u), where @q denotes the Watson-Crick '\n",
            " 'complementarity function. Thus, an expression which involves only a word u '\n",
            " 'and its complement can be still considered as a repeating sequence. In this '\n",
            " 'context, we define and investigate the properties of a special class of '\n",
            " 'primitive words, called pseudo-primitive words relative to @q or simply '\n",
            " '@q-primitive words, which cannot be expressed as such repeating sequences. '\n",
            " 'For instance, we prove the existence of a unique @q-primitive root of a '\n",
            " 'given word, and we give some constraints forcing two distinct words to share '\n",
            " 'their @q-primitive root. Also, we present an extension of the well-known '\n",
            " 'Fine and Wilf theorem, for which we give an optimal bound.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/5de5e21bffc3ecfd1b8dcf36efd5169667b041f8\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Selective backbone construction for topology control in ad hoc networks\n",
            "\n",
            "Abstract : \n",
            "('A key step in controlling topology in ad hoc networks is the construction of '\n",
            " 'the backbone which is then used to transfer data. Nodes that are not part of '\n",
            " 'the backbone can then go to sleep to save energy and increase the lifetime '\n",
            " 'of the network. Centralized backbone construction algorithms give better '\n",
            " 'performance but incur high communication overhead, while localized '\n",
            " 'algorithms lack sufficient topology information needed to construct '\n",
            " 'efficient backbones. We present selective backbone construction (SBC) which '\n",
            " 'starts by selecting a small number of seed nodes in the backbone and then '\n",
            " 'completes its construction by making a sweep of the network spreading '\n",
            " 'outwards from the seed nodes. During the latter process, topology '\n",
            " 'information is transferred to allow better coordinator selection decisions. '\n",
            " 'We compared SBC with other power-saving protocols in a variety of tests '\n",
            " 'featuring different mobility levels, traffic patterns, and node densities. '\n",
            " 'Our experiments show that SBC is more efficient in saving energy and '\n",
            " 'extending network life while providing satisfactory network performance when '\n",
            " 'compared with 802.11, 802.11 PSM, and GAF.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/76058f93bc3b0c0c8491dfdd411f10592f81bc77\n",
            "---\n",
            "-------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}